Kubernetes

Open source container orchestration tool developed by Google
Containers ideal for microservices

High Availability
Scalability
Disaster Recovery

~/.kube/config (default location)
	defines cluster you can access

Check version of K8s 
	kubectl get nodes

Node
	K8s Server
	"Worker Nodes" and "Master Nodes" - see below
	Worker Node contains multiple Pods


Pod
	smallest unit of K8s
	abstraction over a container
		technically 2+ containers
			includes instance of "pause" container 
				k8s.gcr.io/pause-amd64
				for setup/teardown
			may also want sidecar container
				logging/monitoring
				will apply to all containers in that namespace
		running environment (not Docker-specific)
	allows admin to only interact with K8s
	usually contains a single application/container

	each Pod gets own private IP address for communication with other Pods in the same node
		new IP generated each time a Pod is started


Service
	permanent IP address that can be attached to a Pod
	lifecycle of Pod and Service not connected
		Service will stay even if Pod dies

	Internal Service
		just for connections within Node

	External Service
		opens pod to connections outside of Node (e.g. Internet)

		Provides IP of Node and a port for the Pod


Ingress
	Acts as intermediary between Internet FQDN and Service IP


ConfigMap:
	configuration for parameters external to the application - potentially changing resources (e.g. db service endpoint external to application)

	Don't put credentials in ConfigMap:


Secret:
	sim ConfigMap, but used to store secret data
	b64-encoded JSON - meant to be encrypted with 3rd party tools

	not enabled by default

	application can also access this for environment variables or as a properties file


Volumes
	persistent data storage
	can be inside the Node or external (e.g. cloud storage)
	K8s doesn't manage data persistence


Cluster
	contents of a Node or redundant/replica Nodes


Redundancy
	Replicate Pods between multiple Nodes
		define blueprint of Pods and number of replicas req'd
	Uses the same Service for duplicated pods
		load balances


Deployment:
	blueprint for application Pods
	layer of abstraction on top of Pods
		easier to configure, deploy, and replicate Pods

	Deployment is for stateless Pods only!
		can't be used for db Pods - replicas would all need to access same shared data storage (they are stateful)


StatefulSet
	prevents data inconsistencies for multiple db Pods and other stateful applications

	setup is complex - for this reason dbs are often hosted outside of the K8s cluster


DaemonSet
	sim Deployment/StatefulSet
	calculates how many Replicas are needed based on existing Nodes
		no need to defined Replica count, will scale up/down automatically 
		max 1 Replica per Node


Worker Node
	contains 1+ Pods
	cluster servers that do the actual work
	requires:
		container runtime (e.g. containerd, cri-o, Docker)
		Kubelet
			interacts with both container runtime and Node
			starts the Pod with a container inside
		Kube proxy
			forwards requests to/from Services
			keeps requests within Node 
				reduces overhead - e.g. queries db in own node, instead of in replica's node
			maintains list of Service IPs and corresponding Pod IPs


Master Node (Control Plane)
	manages Worker Nodes
	requires:
		API Server
			gateway b/w admin client (e.g. kubectl) & cluster
			does authentication, if enabled
		Scheduler
			receives orders from API Server & Controller Mgr
			only decides where a new Pod should be scheduled
				based on current Node resource availability
			actual scheduling is done by Kubelet inside Node
		Controller Manager
			detects cluster state changes (e.g. dead Pods)
				gets info from etcd
			sends order to Scheduler to react to changes
				e.g. start up a new Pod
		etcd
			key value store of a cluster state (cluster brain)
			stores the information required for API Server, Scheduler, and Controller Manager to work

			does not store application data

	often redundant itself with load balanced API Server and shared information for etcd


Minikube
	1-Node cluster running both master and worker processes
	runs through VBox
	local cluster setup for testing and development
	kubectl is a dependency

	Start up minikube cluster
		minikube start --vm-driver=<hypervisor>


Kubectl
	CLI client for connection to K8s API Server
	most powerful of the K8s clients

	extensions available via plugin
	no command completion by default
		easy to set up in Linux (use WSL on Windows)
			sudo kubectl completion bash > /etc/bash_completion.d/kubectl
	kubectl proxy can expose K8s API to localhost
		good for dev and prototyping

	Get status of cluster Nodes
		kubectl get nodes

	Get client and server versions of K8s in cluster
		kubectl version

	Get status of Services
		kubectl get service

		should always see kubernetes service (default)

	Get status of Pods
		kubectl get pods

	Get IP of Pods
		kubectl get pod -o wide

	Monitor progress of Pod creation
		kubectl get pod --watch 

	Get status of Deployments
		kubectl get deployment

	Get status of Replicas
		kubectl get replicaset

	Create a Pod (via Deployment)
		kubectl create deployment <pod_name> --image=<image_name>

		resulting pod will be named:
			<deployment_name>-<replicaset_ID>-<pod_ID>

	Configure a Deployment
		kubectl edit deployment <deployment_name>

		opens auto-generated config file wil defaults
		save to save changes and update Pod(s)

	Get Pod's application logs
		kubectl logs <pod_name>

	Get Pod's details and state changes
		kubectl describe pod <pod_name>

	Get Service's details and state changes
		kubectl describe service <service_name>

	Check for Secrets created
		kubectl get secret

	Open terminal of Pod
		kubectl exec -it <pod_name> -- bin/bash

	Delete Pod (via Deployment)
		kubectl delete <deployment_name>

	Use a K8s configuration file to customise a Deployment
	& Update a Deployment from a config file
		kubectl apply -f <config-file.yaml>

		K8s knows whether to create or update the deployment

	Get a K8s config file for a deployment in YAML format
		kubectl get deployment <deployment_name> -o yaml > <config-file.yaml>

		Need to remove status: sections before this can be used to apply as a new/updated configuration

	Get security contexts for all pods in a cluster
		kubectl get pods --all-namespaces -o go-template --template='{{range .items}}{{"pod: "}}{{.metadata.name}}
	    {{if .spec.securityContext}}
	      PodSecurityContext:
	        {{"runAsGroup: "}}{{.spec.securityContext.runAsGroup}}                               
	        {{"runAsNonRoot: "}}{{.spec.securityContext.runAsNonRoot}}                           
	        {{"runAsUser: "}}{{.spec.securityContext.runAsUser}}                                 {{if .spec.securityContext.seLinuxOptions}}
	        {{"seLinuxOptions: "}}{{.spec.securityContext.seLinuxOptions}}                       {{end}}
	    {{else}}PodSecurity Context is not set
	    {{end}}{{range .spec.containers}}
	    {{"container name: "}}{{.name}}
	    {{"image: "}}{{.image}}{{if .securityContext}}                                      
	        {{"allowPrivilegeEscalation: "}}{{.securityContext.allowPrivilegeEscalation}}   {{if .securityContext.capabilities}}
	        {{"capabilities: "}}{{.securityContext.capabilities}}                           {{end}}
	        {{"privileged: "}}{{.securityContext.privileged}}                               {{if .securityContext.procMount}}
	        {{"procMount: "}}{{.securityContext.procMount}}                                 {{end}}
	        {{"readOnlyRootFilesystem: "}}{{.securityContext.readOnlyRootFilesystem}}       
	        {{"runAsGroup: "}}{{.securityContext.runAsGroup}}                               
	        {{"runAsNonRoot: "}}{{.securityContext.runAsNonRoot}}                           
	        {{"runAsUser: "}}{{.securityContext.runAsUser}}                                 {{if .securityContext.seLinuxOptions}}
	        {{"seLinuxOptions: "}}{{.securityContext.seLinuxOptions}}                       {{end}}{{if .securityContext.windowsOptions}}
	        {{"windowsOptions: "}}{{.securityContext.windowsOptions}}                       {{end}}
	    {{else}}
	        SecurityContext is not set
	    {{end}}
		{{end}}{{end}}' > output.file


	port-forwarding
		get direct access to a pod's services from a local machine
			portforward cluster services to the machine you run kubectl on
			kubectl port-forward pod/<pod_name> --address <destination_IP> <destination_port>:<pod_service_port>
		good for getting screenshots

	file copying
		copy files out of a container
			kubectl cp default/<pod_name>:/path/to/file/in/container /path/to/copy/destination
		good for collecting evidence for reporting

	verbosity
		higher verbosity level is more information (max is 9, 7 is good)
			e.g. kubectl get pod -v7

	explain
		get documentation on any API resources (find resources using kubectl api-resources)
		can provide template of all available fields for a resource (sim PS help)
			e.g. kubectl explain podsecuritypolicy
		can drill down into object components
			e.g. kubectl explain podsecuritypolicy.spec
		can get additional information
			e.g. kubectl explain podsecuritypolicy.spec --recursive 

	krew
		plugin manager for kubectl (sim pip)
		plugins aren't validated for security - not entirely reliable


YAML Basics
	serialisation language (sim XML, JSON)
	validate YAML at onlineyamltools.com/edit-yaml

	#comments 
	- list of objects or simple items
		simple items can also use key: [item1, item2, item3]
	yes/no, on/off booleans

	multi-line string
		key: |
			value strings on multiple lines
			each one on their own line

	multi-line string wrap
		key: >
			these vlaue lines should be together
			but for ease of reading we're typing 
			them on multiple lines here

	$ENVIRONMENT_VARIABLES

	placeholders - used for Helm and other components
		{{ .Values.service.name }}

	Can put multiple configurations in one file
		separate then with a new line containing ---


K8s YAML Config File Components Example
	#Need to look up apiVersion for each different kind of component created by config file
	apiVersion: apps/v1
	
	#Kind of component to be created (e.g. deployment, service, secret, configmap, ...)
	kind: Deployment
	
	#Metadata of the component to be created (e.g. deployment name, type, ...)
	metadata:
	  name: deployment_name
	  labels:
		app: app_name
	
	#Specifications of the component to be created
	spec:
	  replicas: #_of_replicas
	  selector:
		matchLabels:
		  app: app_name
		template:
		  metadata:
			labels:
			  app: app_name
			spec:
			  containers:
			  - name: container_name
			    image: image_name
				ports:
				- containerPort: 80
				volumeMounts:
				- name: volume_name
				  mountPath: /path/to/directory/to/be/mounted
				env:
				- name: app_username
					value: user_name
				- name: app_password
				  valueFrom: 
				  	secretKeyRef: 
				  		name: secret_name
				  		key: secret_key
				  	#must create secret first to have this reference
				  	#this keeps the secret in K8s instead of in the code repo

	#K8s compares desired state to current state - updated by K8s' etcd continuously
	status:
	  available replicas:
	  conditions:
			...
	#can check status using `kubectl get <resource> <resource-name> -o yaml`


K8s YAML Config Files
	store config files with application code or in own git repo

	Important Sections
		Metadata
		Specifications
		Status

	Template:
		config within config
			applies to the component (e.g. Pod) created
		blueprint for the component
	
	Metadata:
		Labels:
			any key-value pair 
			Deployment connects all components with same label to one another

	Specification:
		Selectors:
			Service connects to Deployment/Pods with same Label 

	Ports for Deployments
		containerPort:
			port on the container/Pod that is open for the Service to connect to

	Ports for Services
		port:
			port on the Service that is open for the container/Pod to connect to
		targetPort:
			The open port on the connected container/Pod that the Service needs to communicate with
			- this is the "Endpoints" property from:
				kubectl get service <service_name>


Example Configuration File for Secret:
	apiVersion: v1
	kind: Secret
	metadata:
	  name: <secret_name>
	type: Opaque 	#Opaque is default for key-value pairs
		#can also be TLS certificate and more
	data:
	  secret_key: b64_secret_value
		#secret values must be b64-encoded

	Just keeps secrets out of codebase


Example Configruation File for External Service:
	apiVersion: v1
	kind: Service
	metadata: 
	  name: service_name
	spec:
	  selector:
	  	app: app_name
	  type: Loadbalancer  
	  	#technically internal is load-balanced, too, but this is the name for the external service type - assigns an external IP
	  ports:
	    - protocol: TCP
	      port: 8081
	      targetPort: 8081 
	      nodePort: 30001
	      	#this is port where the service will be accesible on the Node's external IP
	      	#must be between 30000 and 32767


Quick manifest generation
	Resources (Services, Deployments, etc.)
		kubectl create <resource> <resource_name> [<other_required_parameters>] --dry-run=client -o yaml > file.yml
	Pods
		kubectl run <pod_name> --image=<image_name> [<other_required_parameters>] --dry-run=client -o yaml > file.yml


Installation from Scratch
	on each server (control plane and worker) from package repos:
		Container Runtime
		Kubelet
		(Kube Proxy)
	on control plane from manifest files:
		API
		Scheduler
		Controller Manager
		etcd

		installed as static Pods
			initially, control plane pods need to be installed without Scheduler, etc (which is a control plane pod)
			managed directly by kubelet daemon, without control plane (Controller Manager, Scheduler)
				Kubelet continuously watches /etc/kubernetes/manifests/ on its Node for any manifests
				schedules any pods with manifests there
			suffixed with name of node they are running on
			get IP of control plane node they are running on


Installation Process:
	install runtime on all nodes
	install kubelet, kubeadm, and kubectl on all nodes
		release cycle of kubelet, kubeadm, and kubectl are sync'd (and must be the same on the install)
			check available versions using: `apt-cache madison kubeadm`
			install specific version using: `sudo apt-get install -y kubelet=<version> kubeadmin=<version> kubectl=<version>`
	initialise control-plane node using `kubeadm init`
		generates CA
		generates and places certs
		generates static pod manifests in /etc/kubernetes/manifests/
		makes all necessary configs to initialise control plane
		installs CoreDNS and kube-proxy
	set kubeconfig as user file at ~/.kube/config (or temporarily as env KUBECONFIG)
		copy /etc/kubernetes/admin.conf to ~/.kube/config
	configure namespaces
	install CNI on control plane node
	configure firewall rules for networking plugin
	join worker nodes to cluster using kubeadm join command 
		provided following kubeadm init
		can generate new connection command from `kubeadm token create --print-join-command`

Container Runtime Interface (CRI)
	generic plugin interface for Kubelet to interact with any container runtime
		Docker didn't have CRI compatibility (originally part of Kubelet code)
			fixed using dockershim layer between CRI and Docker runtime
				dockershim deprecated in 1.20, removed in 1.22
		better to use lightweight container runtimes like containerd and cri-o
			will still run Docker images
			EKS, AKS, GKE all use containerd in managed clusters
			no Docker commands available, but preferable to work with K8s commands anyways	


Signatures and Encryption
	1) self-signed CA cert for K8s
	2) use to sign Scheduler, Controller Manager, Kubelet client certs and API Server server cert
	3) and to sign API Server client cert and etcd, Kubelet server certs
		can be same as API Server server cert, but best practice is to generate new client cert
	4) store certs in /etc/kubernetes/pki/


kubeadm
	toolkit for bootstrapping K8s clusters
		will create admin user auth certs, though.
	need to turn off swap on all nodes - `sudo swapoff -a`
	need to open ports on control plane
		6443 from ANY
		2379-2380, 10250, 10251, 10252 from Cluster
	need to open ports on worker nodes
		10250 from Cluster
		30000-32767 from ANY


Authorization Modes
	Node
	ABAC
	RBAC
	Webhook

	can enable multiple modes in API Server config /etc/kubernetes/manifests/kube-apiserver.yaml
		--authorization-mode  (Node and RBAC by default with kubeadm)

RBAC
	Role component defines namespaced permissions
		bound to specific namespace
		resources accessible and actions allowable
	RoleBinding component defines which users and groups receive a Role
	
	ClusterRole component defines cluster-wide permissions
		not limited to a specific namespace
	ClusterRoleBinding defines which users and groups receive a ClusterRole

	Human Users
		User/Group definition
			Static token file - can include group info 
			Certs - manually generated for each user
			3rd-party (e.g. LDAP)

		API Server uses these definitions to authn and authz

	Service Accounts
		K8s component for authn and authz of apps inside and outside cluster
		
		kubectl create serviceaccount <account_name>

			creates "system:serviceaccount:<namespace>:<account_name>"

		Token Controller watches ServiceAccount creation and generates API token

	Role
		kind: Role
		metadata:
		  name:
		  namespace: <namespace> 	#can limit access to specific namespace(s)
		rules:
		- apiGroups: [""]	#"" is Core API group
		  resources: ["<resource_type"]
		  verbs: ["get", "create", "list", "delete", "update"]
		  resourceNames: ["<resource_name>"] 	#can limit access to specific named resource instances

		kubectl create role <role_name> --verb=<allowed_verbs> --resource=<resources> --dry-run=client -o yaml > role_filename.yml
			may need to check K8s documentation to see which API each resource belongs to and which verbs are available

		kubectl create role cicd-role --verb=create,update,list --resource=deployments.apps,services --dry-run=client -o yaml > cicd-role.yml


	RoleBinding
		kind: RoleBinding
		subjects:
		- kind: <User or Group or ServiceAccount>
		  name: <username or groupname or default>
		  apiGroup: rbac.authorization.k8s.io  #`namespace: kube-system` instead for ServiceAccounts
		roleRef:
		- kind: Role
		  name:
		  apiGroup: rbac.authorization.k8s.io

		kubectl create rolebinding <rolebinding_name> --role <role_name> [--user=<username>] [--group=<group_name>] [--serviceaccount=<namespace>:<serviceaccountname>] --dry-run=client -o yaml > rb_filename.yml

	ClusterRole
		kind: ClusterRole
		rules:
		- apiGroups: [""]	#"" is Core API group
		  resources: ["<resource_type>"]
		  verbs: ["get", "create", "list", "delete", "update"]

		kubectl create clusterrole <cr_name> --verb=<allowed_verbs> --resource=<resources> --dry-run=client -o yaml > cr_filename.yml
			check K8s documentation to see which API each resource belongs to and which verbs are available
		
	ClusterRoleBinding
		kind: ClusterRoleBinding
		subjects:
		- kind: <User or Group or ServiceAccount>
		  name: <username or groupname or default>
		  apiGroup: rbac.authorization.k8s.io  #`namespace: kube-system` instead for ServiceAccounts
		roleRef:
		- kind: ClusterRole
		  name:
		  apiGroup: rbac.authorization.k8s.io

		kubectl create clusterrolebinding <cr_name> --clusterrole <clusterrole_name> [--user=<username>] [--group=<group_name>] [--serviceaccount=<namespace>:<serviceaccountname>] --dry-run=client -o yaml > crb_filename.yml


Client Certificates
	stored in /etc/kubernetes/pki and component config files (e.g. /etc/kubernetes/kubelet.conf)
	kubeadm generates 2 CAs (kubernetes-ca and etcd-ca)
		cluster CAs are signed by copy of K8s' own cert

	Signing a Client Cert
		1) Create a Key-Pair
			openssl genrsa -out <key-name>.key 2048
		2) Generate Certificate Signing Request
			openssl req -new -key <key_name>.key -subj "/CN=<username>" -out <CSR_filename>.csr
		3) Send CSR using K8s Cert API (cluster root CA = kubernetes-ca)
			CSR.yaml
				apiVersion: certificates.k8s.io
				kind: CertificateSigningRequest
				metadata: 
				  name: <username>
				spec:
				  request: <base64-encoded_CSR_data> 	#cat <CSR_filename>.csr | base64 | tr -d "\n"
				  signerName: kubernetes.io/kube-apiserver-client
				  expirationSeconds: 86400 #one day
				  usages:
				  - client auth
			kubectl apply -f CSR.yml
		4) K8s admin approves cert signing request
			kubectl certificate approve <username>
		5) K8s admin creates certificate file
			kubectl get csr <username>
			copy the certficate: content
			echo "<copied_cert_content>" | base64 -d > username.crt
		6) Provide CRT and kubeconfig file (see below) to user

	Connect to Cluster using Client Cert
		NOTE: ~/.kube/config will override these parameters if logged in as admin

		find API server address using `kubectl cluster-info`
		kubectl <command> <resource> --server https://<API_Server_IP>:6443 --certificate-authority /etc/kubernetes/pki/ca.crt --client-certificate "/path/to/cert_file.crt" --client-key "/path/to/privatekey_file.key"

		OR include the cert info in kubeconfig file for this user
			can b64-encode cert and key contents in client-certificate-data: and client-key-data sections
			OR
			use file paths in client-certificate: and client-key: sections

			kubectl <command> <resource> --kubeconfig </path/to/user/kubeconfig/file>

	Connecting to Cluster using API Token
		get token from secrets and save as variable $token

		kubectl <command> <resource> --server=https://<API_Server_IP>:6443 --certificate-authority /etc/kubernetes/pki/ca.crt --token $token

		OR include the cert info in kubeconfig file for this user
			remove client-certificate-data: or client-certificate:
			remove client-key-data: or client-key:
			replace with
				user:
				  token: <serviceaccount_JWT>

			kubectl <command> <resource> --server=https://<API_Server_IP>:6443 --certificate-authority /etc/kubernetes/pki/ca.crt --kubeconfig=</path/to/kubeconfig/file>

	Managing and Renewing Certs
		client certs valid 1 year
		CA certs valid 10 years

		check cert expiration
			sudo kubeadm certs check-expiration
			OR
			openssl x509 -in /etc/kubernetes/pki/<cert>.crt -text -noout	#more info from this command

		renew certs
			NOTE: regular cluster upgrades will include cert renewal automatically
			see `sudo kubeadm certs renew --help`


Namespaces
	logical unit for organising resources within a cluster
	a virtual cluster within a cluster
	
	K8s provides 4 namespaces by default
		default
			where resources are created by default
		kube-node-lease
			info about node heartbeats - availability
		kube-public
			publicly accessible data
			configmap contains cluster info
			no authentication
		kube-system
			for Master & kubectl system processes/pods
			do not create or modify 

	Get list of namespaces
		kubectl get namespace
		kubectl get ns

	Get list pods in a namespace
		kubectl get pod -n <namespace>

	Create a namespace
		kubectl create namespace <namespace_name>
		OR use namespace config file

	Can also create namespaces through config files
		apiVersion: v1
		kind: ConfigMap
		metadata: 
		  name: config_map_name
		  namespace: namespace_name
		data:
		  ...

	Use Cases
		Everything in one namespace
			very confusing, poor overview
		Group resources into their own namespaces
			e.g. group all dbs together, all montoring, all logging, all web apps, etc
		Conflicts: Many teams, same application
			prevent deployments with same name but different configurations from overwriting one another
		Resource Sharing: Staging and Development
			can deploy a common resource once for use by both staging and development
		Resource Sharing: "Blue/Green" Deployment
			want 2+ different Production versions in use and may require access to common shared resources
		Access and Resource Limits on Namespaces
			for multiple teams - limit access to only the resources in the team's namespace
			limit resources (CPU, RAM, storage) consumed by each namespace individually (resource quotas)

	K8s does not recommend using addtional namespaces for smaller projects and fewer than 10 users
		but it's still useful

	Can't access MOST resources from another Namespace
	Each namespace must have its own:
		ConfigMaps
		Secrets
	Can share:
		Services
			reference to a shared service: <service_name>.<namespace>
	SOME components cannot be confined to a namespace
		Volume
		Node

	List all resources bound to a namespace
		kubectl api-resources --namespaced=true
	List all resources NOT bound to a namespace
		kubectl api-resources --namespaced=false

	List all resources in a namespace
		kubectl get all -n <namespace>

	Get resource from specific namespace
		kubectl get <resource_type> -n <namespace>

	Create a resource in a sepcific namespace
		kubectl apply -f config-file.yaml --namespace=<namespace>
		OR
		use namespace: under metadata: for resource within the config file
		e.g. namespace: <namespace>

	Change "Active" (default) namespace using kubens
		kubens <namespace>

		no longer need to provide -n <namespace> if working in one namespace other than K8s' "default" ns


Labels 
	check using:
		kubectl describe <resource> <resource_name>
		OR
		kubectl get <resource> --show-labels
	select resource instances by label
		kubectl get <resource> -l label_key=label_value
	Use Cases:
		1) Connecting Services to Pods
			selector in Service manifest to match label in Pod manifest
		2) General Housekeeping


Scaling
	adjust replicaCount in deployment file
	OR
	kubectl scale <resource> <resource_name> --replicas=<#_of_replicas>


Multi-container Pods
	helper apps
	can talk to main application using localhost

	must specify which container in kubectl commands
		e.g. kubectl logs nginx-deployment-234sdfnsdf-sdfsf log-sidecar

	Sidecar Container
		interacts continuously with main app
		e.g. log collection, db sync
		defined as additional container in containers: section of Pod or Deployment manifest

	Init Container
		interacts just once with main app at beginning and then exits
		e.g. startup scripts/checks
		defined in initContainers: section of Pod or Deployment manifest

		pod will remain on status Init:x/y until checks/scripts are completed and main app starts

	Intra-pod communication 
		all containers have access to information about their Pod
		can expose Pod fields using Environment Variables or Volumes

		Environment Variable
			add to container spec in Pod or Deployment manifest
			env:
			- name: ENV_VARIABLE_NAME
			  value: <static_value>		
			OR
			env:
			- name: ENV_VARIABLE_NAME
			  valueFrom:
			    fieldRef:
			      fieldPath: <hierarchical_data_location> #e.g. status.podIP


Rolling Updates
	avoids application downtime during redeployment and allows for rollback in case of issues

	Deployments create an application rollout and create a ReplicaSet (automatically in background)
	
	ReplicaSets ensure that a specified number of Pod replicas are running at any given time

	When Deployment is updated, a new ReplicaSet is created
		new Pods get created in new ReplicaSet, old Pods get deleted
		old ReplicaSets remain but without any Pods

	1) Recreate Strategy
		kill all old pods before recreating new ones -> application downtime
	2) Rolling Update Strategy
		kills one old pod before recreateing new one untill all old gone and all new in place
		default strategy for K8s

	configured in Deployment manifest

	spec:
	  strategy: 
	    type: RollingUpdate
	    rollingUpdate: 
	      maxUnavailable: .25		#max proportion of pods unavailable during update
	      maxSurge: .25 	 		#max proportion of pods that can be created over desired number of pods

	check rollout/update history
		rollout creates "Revisions"
		kubectl rollout history deployment <deployment_name>

	Rollback
		kubectl rollout undo deployment <deployment_name>

etcd Backup and Restore
	Install etcdctl
		sudo apt install etcd-client
	Find etcd certs
		sudo cat /etc/kubernetes/manifests/etcd.yaml | grep /etc/kubernetes/pki
		get ca cert, server cert, and server key locations
			will need to have these available if working from somewhere other than control-plane node

	Backup
		sudo ETCDCTL_API=3 etcdctl snapshot save /path/to/save/backup.db --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.cert --key /etc/kubernetes/pki/etcd/server.key

	Restore
		sudo ETCDCTL_API=3 etcdctl snapshot restore /path/to/save/backup.db --data-dir /var/lib/etcd-backup

		update etcd-data volume in /etc/kubernetes/manifests/etcd.yaml to /var/lib/etcd-backup

	Options to manage etcd external to cluster
		1) Use remote storage outside cluster
			change volume: for etcd-data in /etc/kubernetes/manifests/etcd.yaml to remote storage
		2) Run etcd outside cluster
			more complex but more secure


K8s REST API
	1) can set up kubectl to act as a reverse proxy for queries
		uses stored apiserver location and cert 
	
		kubectl proxy --port=<port_to_access_proxy_on> &
		curl http://localhost:<selected_port>/api/v1/...

	2) can also access without kubectl
		need to pass authn data for authenticated user in cluster
			preferably a service account with limited permissions
			save token as env TOKEN
		need server location
			kubectl config view
			save server URL as env SERVER

		curl -X GET $SERVER/api/... -H "Authorization: Bearer $TOKEN" --cacert /etc/kubernetes/pki/ca.crt
		curl -X GET $SERVER/api/... -H "Authorization: Bearer $TOKEN" --insecure  #no server cert validation

	3) programmatic access 
		officially supported client libraries for Go, Python, JS, etc


Cluster Upgrades
	When:
		fix in later version that affects workload
		as often as possible
	Upgrade only one version at a time (e.g. 1.19 -> 1.20 -> 1.21 , not 1.19 -> 1.21)

	1) Upgrade Control Plane
		no application downtime on worker nodes, but can't manage/restart components
		should have 2+ control-plane nodes
			upgrade one at a time

		kube-apiserver must be latest version component in cluster
			controller-manager and kube-scheduler can be 1 version behind
			kube-proxy and kubelet can be 2 versions behind
			kubectl can be 1 version ahead or behind

		kubeadm will upgrade most basic components (incl etcd and coredns), but not custom interfaces (CNI, etc)
		
		1) upgrade kubeadm tool

		2) upgrade control-plane components and renew certs
			kubeadm upgrade apply <version_number> 	#e.g. 1.21.0

		3) remove all pods from Node and prevent rescheduling
			kubectl drain <node_name>

		4) upgrade kubelet and kubectl
			
		5) change control node back to schedulable
			kubectl uncordon <node_name>
			NOTE: `kubectl cordon` can be used to stop new pods being scheduled on any node

	2) Upgrade Worker Nodes
		with 2+ worker nodes running 2+ pod replicas, there should be no app downtime

		1) upgrade kubeadm tool

		2) upgrade all kubelet configs
			kubeadm upgrade apply <version_number> 	#e.g. 1.21.0

		3) remove all pods from Node and prevent rescheduling - reschedules on a different node
			kubectl drain <node_name>

		4) upgrade kubelet
			
		5) change worker node back to schedulable
			kubectl uncordon <node_name>


Multi-Cluster Management
	kubeconfig file cluster section is a list
		clusters:
		- cluster: 
		    certificate-authority-data: ...
		    server: <server1_URL>
  		  name: cluster_name1
		- cluster: 
		    certificate-authority-data: ...
		    server: <server2_URL>
  		  name: cluster_name2

	kubeconfig file also defines lists of users and contexts for clusters
	i.e. which user(s) should be used to access which cluster(s) (and namespaces)
		contexts:
		- context:
		    cluster: cluster_name1
		    namespace: <namespace>							#"default", if this line not present
		    user: <cluster_user1>
		  name: <context_name1> 							#e.g. <cluser_user>@<cluster_name>
		- context:
		    cluster: cluster_name2
		    user: <cluster_user2>
		  name: <context_name2>								#e.g. <cluser_user>@<cluster_name>
		current-context: <current_context_being_used>		#change on the fly using kubectl config
		users:
		- name: <cluster_user1>
		  user:
		    client-certificate-data: ...
		    client-key-data: ...
		- name: <cluster_user2>
		  user:
		    client-certificate-data: ...
		    client-key-data: ...

	can add users/contexts/clusters manually or kubectl
		see `kubectl config --help`

	see all contexts
		kubectl config get-contexts

	see current context
		kubectl config current-context

	change current-context
		kubectl config use-context <context_name>

	change current users's default namespace for kubectl commands (from "default" to another namespace)
		kubectl config set-context --current --namespace=<namespace>




Resource Requests and Limits
	defined in Pod/Deployment manifest
		spec:
		  containers:
		    resources:
		      requests:
		        cpu: "250m"		#250 millicores = 0.25 CPU
		        memory: "64Mi"	#64 megabytes
		      limits:
		        cpu: "500m"
		        memory: "128Mi"

	check limits
		kubectl get pods -o jsonpath="{range .items[*]} {.metadata.name}{.spec.containers[*].resources}{'\n'}"

	pods without explicit resource definitions will be evicted first in case of resource overuse


Scheduling Pods
	override Scheduler's optimised assignments
	e.g. restricting a node to specific pods, restricting number of replicas per node
	interfere with K8s scheduling as little as possible

	Assigning Pods to Nodes
		NodeName - for staticly named nodes
			spec:
			  containers:
			  - name: container_name
			    image: image_name
			  nodeName: <name_of_node_to_schedule_to>

	    NodeSelector - for dynamically named nodes
	    	1) label the Nodes
	    		kubectl label node <node_name> <labelkey>=<label_value> 
	    	2) add nodeSelector field to Pod/Deployment config
	    		spec:
	    			containers:
	    			- name: container_name
	    			  image: image_name
	    			nodeSelector:
	    			  <label> #e.g. disktype: ssd

	    	if not enough resources, the Pods will not be scheduled

	Node Affinity
		more flexible logical operators than NodeSelector
			In, Not In, Exists, DoesNotExist, Gt, Lt, 
			requiredDuringSchedulingIgnoredDuringExecution: - hard rule
				will not schedule elsewhere if requirements not met
			preferredDuringSchedulingIgnoredDuringExecution: - soft rule
				will schedule elsewhere if preferences not met

			spec:
			  containers:
			  - name: container_name
			    image: image_name
			  affinity:
			    nodeAffinity:
			      requiredDuringSchedulingIgnoredDuringExecution:
			        nodeSelectorTerms:
			        - matchExpressions:
			          - key: node_label_key
			            operator: In
			            values:
			            - node_label_value1
			            - node_label_value2
				  AND/OR
				  preferredDuringSchedulingIgnoredDuringExecution:
			        weight: 1
			        preference:
			        - matchExpressions:
			          - key: another_node_label_key
			            operator: In
			            values:
			            - another_node_label_value
			            
	Taints & Tolerations
		configure node itself for preferential workloads instead of configuring the Pods/Deployments
		taints prevent types of pods being scheduled there (e.g. any pods prevented from control-plane node)
			check taints using `kubectl describe node | grep Taint`
		tolerations override taints for specific exceptions (e.g. logging pods only)
			check node tolerations using `kubectl get node -o json | grep Toleration`
			check pod tolerations using 
				kubectl get pod -o custom-columns=POD_NAME:.metadata.name,TOLERATIONS:.spec.tolerations

		Toleration for Master Taint
			spec:
			  containers:
			  - name: container_name
			    image: image_name
			  tolerations:
			  - effect: NoExecute 	#Taint
			    operator: Exists
			
			would also need NodeName/NodeSelector/Affinity to schedule on control-plane node specifically

	Inter-Pod Affinity/Anti-Affinity
		can constrain Pod assignment based on (labels on) Pods already running on a Node
			e.g. don't run a replica of this Pod on a Node already running this Pod
			e.g. only run this Pod on a Node running a Pod with a specific label
		similar to DaemonSet functionality

			Inter-Pod Affinity (e.g. single replica only on Master Nodes)
				spec:
				  containers:
				  - name: container_name
				    image: image_name
				  nodeSelector:
				    type: master
				  tolerations:
				  - effect: NoExecute
				    operator: Exists
				  affinity:
				    podAffinity:
				    	requiredDuringScheduling...		#same as NodeAffinity rules but with Pod labels
				    podAntiAffinity:
				    	requiredDuringScheduling...		#same as NodeAffinity rules but with Pod labels


Liveness and Readiness
	can alert on container problem without pod issue - allow for restarts and troubleshooting

	Readiness Probe
		Did the application in the container run successfully?
		Is it ready to receive traffic?

		1) Exec probe
			spec:
			  containers:
			  - name: container_name
			    image: image_name
			    readinessProbe:
			      periodSeconds: 5
			      exec:
			        command: ["cat /tmp/healthy"] 	#executes specified command to check health
				  initialDelaySeconds: 10

		2) TCP probe
			spec:
			  containers:
			  - name: container_name
			    image: image_name
			    ports:
			    - containerPort: 80
			    readinessProbe:
			      periodSeconds: 5
			      tcpSocket:
			        port: 80 					#makes probe connection at the node (not in Pod)
			      initialDelaySeconds: 10

		3) HTTP probe
			spec:
			  containers:
			  - name: container_name
			    image: image_name
			    readinessProbe:
			      periodSeconds: 5
			      httpGet:
			        path: /health 		#sends HTTP request to specified path and port for application
			        port: 3000
				  initialDelaySeconds: 10


	Liveness Probe
		Is the application in the container still running successfully?

		1) Exec probe
			spec:
			  containers:
			  - name: container_name
			    image: image_name
			    livenessProbe:
			      periodSeconds: 5
			      exec:
			        command: ["cat /tmp/healthy"] 	#executes specified command to check health

		2) TCP probe
			spec:
			  containers:
			  - name: container_name
			    image: image_name
			    ports:
			    - containerPort: 3000
			    livenessProbe:
			      periodSeconds: 5
			      tcpSocket:
			        port: 3000 	#makes probe connection at the node (not in Pod)

		3) HTTP probe
			spec:
			  containers:
			  - name: container_name
			    image: image_name
			    livenessProbe:
			      periodSeconds: 5
			      httpGet:
			        path: /health 	#sends HTTP request to specified path and port for application
			        port: 3000


Records
	imperative commands (i.e. kubectl instead of declarative manifests) not recorded anywhere by default
	use `--record` option to record imperative activities
	check records using `kubectl rollout history <resource> <resource_name>


DNS
	/etc/hosts
	CoreDNS 
		prior to 1.12, DNS was kube-dns - this name has stuck for DNS Service name
		store DNS server IP in /etc/resolv.conf
			generated by Kubelet when pod is created
				set in /var/lib/kubelet/config.yaml
			default 10.96.0.10
			"search" parameter in resolv.conf lists domains/namespaces the DNS will resolve without FQDN
		each namespace is its own subdomain
			FQDN = <service_name>.<namespace>.svc.cluster.local
			within the domain, service name is enough

Networking
	every pod has a unique IP address reachable from all other pods in the cluster
	to avoid port conflicts between similar applications on the same node, bind to different ports
		difficult to keep track of which ports are used
	K8s avoids need for tracking ports by using pod abstraction
		pod treated as host, with own set of ports - only need to worry about containers in pod (usually just 1-2)
		containers inside pod talk to one another using localhost:port
	"Pause" Container
		1 per pod
		"sandbox" container to reserve network namespace (netns) to hold pod's IP address
	no built-in pod-to-pod networking solution in K8s - need CNI
	Container Networking Interface (CNI)
		requires:
			every pod gets own IP 
			pods on same node can communicate with that IP
			pods on diff nodes can communicate with that IP without NAT
		examples - flannel, cilium, weavenet
	IntraNode Communication
		node is assigned an IP in the LAN
			eg. LAN = 172.31.0.0/16
			    	Node1 = 172.31.1.10
			    	Node2 = 172.31.1.11
		node is also assiged a separate bridge IP range that does not overlap the LAN
			bridge enables pod communication on same node - sim. switch
			node's bridge range is part of CIDR block for whole cluster, so all pods have unique IPs in that block
			eg. bridge range = 10.32.0.0/12
					Node1 bridge = 10.32.1.0/24
						N1Pod1 = 10.32.1.10
						N1Pod2 = 10.32.1.11
					Node2 bridge = 10.32.2.0/24
						N2Pod1 = 10.32.2.10
	InterNode Communication
		each node has gateway address (its LAN IP)
		servers have routing tables to other nodes
			CNI plugins deployed on each node manage this routing


K8s Network Policy
	network ACLs for clusters
		aware of K8s labels instead of IP addresses
	enabled by default, but default is Allow All
	configures the communication handled by the CNI
		not all CNIs support NetPol (e.g. flannel)

	list current policies
		kubectl get netpol

	example policy
		kind: NetworkPolicy
		apiVersion: networking.k8s.io/v1
		metadata: 
		  name: web-deny-all
		  namespace: default 	#namespace where application being restricted is
		spec:
		  podSelector:			#which applications/pods will this apply to (podSelector: "" applies to whole ns)
		    matchLabels:
		      app: <label1>
		  policyTypes:			
		  	- Ingress			#incoming traffic rules
		  	- Egress			#outgoing traffic rules
		  ingress:
		  - from:
		    - podSelector:
		        matchLabels:
		          app: <label2> 	#allow only "label2" pods to transmit to "label1" pods
		      namespaceSelector:
		        matchLabels:
		          kubernetes.io/metadata.name: <namespace>	#source is in different namespace
		    ports:
		    - protocol: TCP
		      port: <service_port>	#allow only above label2 traffic coming in to label1 pods on specified port
		  egress:				#ONLY APPLIES TO TRAFFIC THE SPECIFIED PODS INITIATE, NOT TO REPLIES!
		  - to:
		    - podSelector:
		        matchLabels:
		          app: label3 		#allow label1 pods to transmit only to label3 pods
		      namespaceSelector:
		        matchLabels:
		          kubernetes.io/metadata.name: <namespace>	#destination is in different namespace

		NOTE: 
			matches podSelector AND namespaceSelector
				- podSelector:
				 ...
				  namespaceSelector:

			matches podSelector OR namespaceSelector
				- podSelector:
				 ...
				- namespaceSelector:
		  
		DENY ALL incoming
			podSelector: {} 	# empty, matches all pods
			policyTypes:
			- Ingress

		ALLOW ALL incoming
			podSelector: {} 	# empty, matches all pods
			policyTypes:
			- Ingress
			ingress:
			- {}

    apply policy
    	kubectl apply -f /path/to/net_pol.yml

    scaling becomes very complex
    some CNIs (Calico, Cilium) offer additional capabilities for network traffic mgmt
    some cloud providers (AWS, Azure) also have specific FW capabilities
    good animated illustrated examples of netpol recipies
    	github.com/ahmetb/kubernetes-network-policy-recipes

Service IPs 
	cluster-ip accessible only within cluster
		default from kubeadm init is 10.96.0.0/12
			can change during init using --service-cidr option for kubeadm
		generated from range defined in /etc/kubernetes/manifests/kube-apiserver.yaml
			--service-cluster-ip-range
			can modify this parameter after cluster creation 
				API server must restart following change
				services created prior to modification/restart will keep old IPs


Exposing application to outside cluster
	NodePort
		external Service (instead of "ClusterIP" type)
			in spec:, add `type: NodePort` (ClusterIP is default Service type)
			in ports:, add `nodePort: <port_number>`
				port_number is externally-accessible port on node
				range is 30000-32767
		messy and opens ports on worker nodes unecessarily
	Loadbalancer
		external Service - creates load balancer outside the cluster and forwards to an equivalent NodePort 
			in spec:, add `type: LoadBalancer` 
			in ports:, add `nodePort: <port_number>`
		requires a load balancer to be created (in cloud platform) for each service
			created automatically in K8s managed services
			on-prem requires load balancer created by admin
			expensive at scale
			still exposes NodePorts - messy and insecure
	Ingress
		load balancing and routing to all Services securely
		still needs to be exposed via LoadBalancer or Proxy Server
			single entry point to Ingress without exposing nodes
		requires:
			Ingress Controller Pod
				entrypoint to cluster - evaluates rules, manages redirections
				K8s Nginx Ingress Controller or 3rd-party controller
				easy to install using Helm

			Ingress resource
				kind: Ingress
				spec: 
					tls:
					- hosts: 
						- <domain_covered_by_cert> 
						secretName: <secret_containing_cert>
					rules:
					- host: <main_web_URL_or_subdomain> #e.g. subdomain.myapp.com
												#this will be an IP address, if web server is outside cluster
					  http:
					    paths:
					    - path: /<page>  #i.e. myapp.com/<page>
					      backend:
					        serviceName: <service_name>
					        servicePort: <service_port>
			Service resource(s)
				default ClusterIP type
			Application pod(s)

		can also add metadata annotations to redirect incoming path to somewhere else
			metadata:
			  annotations:
			  	ngnix.ingress.kubernetes.io/rewrite-target: /new_path
		ideally should have one Ingress pod per worker node



		TLS Cert Secret
	apiVersion: v1
	kind: Secret
	metadata:
		name: <secret_name>
		namespace: <namespace>
	data:
		tls.crt: <base64_encoded_cert>
		tls.key: <base64_encoded_key>
	type: kubernetes.io/tls


Ingress
	External Service provides IP only
	Ingress allows for FQDN
		also does not open the application itself externally
	Requires an Ingress Controller Pod to process rules
		e.g. K8s Nginx Ingress Controller
			3rd-party implementations
		evaluates all rules
		manages redirections
		entrypoint to cluster
	Cloud deployment
		Can place cloud load balancer in front of this
			acts as entrypoint
	Bare Metal deployment
		Can deploy Ingress Controller internal to cluster or external
		Need to configure an entrypoint
			external Proxy Server (software or hardware)
				separate server
				public IP address and open ports
				no direct access to K8s from Internet

	Example Configuration File for Ingress:
	
	apiVersion: networking.k8s.io/v1beta1
	kind: Ingress
	metadata: 
	  name: ingress_name
	spec:
	  rules:			#routing rules
	  - host: app.com  	
	  		#valid domain entrypoint 
	  		#forwards traffic to serviceName
	    http:			
	    	#protocol for comm b/w Ingress & Internal Service 
	    	#not related to protocol for connecting to Ingress
	      paths:		#subdirectory path(s) for FQDN
	      - backend:
	          serviceName: internal_service_name 	
	          		#destination of fwd
	          servicePort: 8080 	#internal service port

	Default Backend
		default-http-backend:80
		used to handle any request not mapped to a specific service
		can customise error responses
			create internal service with same name on port 80
			create application that send the custom response

	Ingress Use Cases
		Multiple paths for same host
			one domain, but many services
				multiple path: entries (endpoints) under same paths: attribute
		Multiple subdomains or domains
			1+ domains and subdomains
				multiple host: entries under rules: attribute

	Configuring TLS Certificate
		Allows HTTPS forwarding 
		
		In Ingress cluster create TLS secret
			apiVersion: v1
			kind: Secret
			metadata:
			  name: secret_name
			  namespace: namespace
			data:
			  tls.crt: b64_encoded_cert
			  tls.key: b64_encoded_key
			type: kubernetes.io/tls

		In Ingress config file's spec: section add
			tls:
			- hosts:
			  - domainname.com
			  secretName: secret_name


Helm
	1) Package Manager for K8s
		package YAML files and distribute them in repos
	  Helm Chart
		package of YAML files for a complex setup
		e.g. spin up Elastic Stack with all StatefulSet, ConfigMap, Secret, Services, K8s Users & permissions

	  Find Helm Charts using Helm
		helm search <package_name>

	2) Templating Engine
		create common blueprint for resource type
			eg. for set of microservices
		dynamic values in the YAML config file are replaced by placeholder values in template file
			eg. name: {{ .Values.name }}
				containers:
					image: {{ .Values.container.image }}
			.Values are defined in values.yaml or using --set
				e.g. helm install -set version=2.0.0
					see Inject values in templates below

	3) Same applications across different environments
		e.g. Dev, Staging, Prod
		package up own chart 

	4) Release Management
		Helm Version 2
			Helm Client 
			Helm Server ("Tiller")
			helm install command sends requests from client (helm CLI) to Tiller
			Tiller stores copy of each configuration sent for future reference - versioning
			helm upgrade <chart_name>
				Changes are applied to current deployment instead of creating a new one
			helm rollback <chart_name>
				Rollback a bad upgrade
			Tiller has too much power in cluster!!
		Helm Version 3
			No Tiller - just Helm binary


Helm Charts
	Example Helm Chart 
	<chart_name>/ 		#top-level folder - name of chart
		Chart.yaml  	#meta info about chart
		values.yaml 	#values for template files
		charts/			#folder for chart dependencies
		templates/		#folder for templates
		...				#other files (eg. Readme, License)

	Create a chart
		helm install <chart_name>

	Inject values into template files
		helm install --values=new-values.yaml <chart_name>

		e.g. use diff template values for Dev and Prod
		will use default template (from Chart) first
		will only override with the provided values
			values.yaml 		#original defined in chart
				imageName: myapp
				port: 8080
			new-values.yaml 	#injected values
				port: 8081
			# resulting .Values object
				imageName: myapp
				port: 8081



Volumes
	for data persistence
	a Pod can use multiple different storage types
	need to be configured by admin before devs deploy pods that require storage

	Persistent Volume (PV)
		cluster resource - interface to physical storage
		needs actual physical storage from local disk or cloud
			managed outside K8s - health, backup
		NOT namespaced!
		diff attributes for diff types of storage

		e.g. NFS backend
			apiVersion: v1
			kind: PersistentVolume
			metadata: 
			  name: volume_name
			spec:
			  capacity:
			    storage: 5Gi
			  volumeMode: Filesystem
			  accessModes:
			    - ReadWriteOnce
			  persistentVolumeReclaimPolicy: Recycle
			  storageClassName: slow
			  mountOptions:
			    - hard
			    - nfsvers=4.0
			  nfs:
			    path: /dir/path/on/nfs/server
			    server: nfs_server_ip_address

		e.g. GCP Storage
			apiVersion: v1
			kind: PersistentVolume
			metadata: 
			  name: volume_name
			  labels:
			    failure-domain.beta.kubernetes.io/zoneL us-central1-a__us-central1-b
			spec:
			  capacity:
			    storage: 400Gi
			  accessModes:
			    - ReadWriteOnce
			  gcePersistentDisk:
			    pdName: data_disk_name
			    fsType: ext4

		e.g. local storage
			apiVersion: v1
			kind: PersistentVolume
			metadata: 
			  name: volume_name
			spec:
			  capacity:
			    storage: 400Gi
			  volumeMode: Filesystem
			  accessModes:
			    - ReadWriteOnce
			  persistentVolumeReclaimPolicy: Delete
			  storageClassName: local-storage
			  local:
			    path: /mnt/disks/ssd1
			  nodeAffinity:
			  	required:
			  	  nodeSelectorTerms:
			  	  - matchExpressions:
			  	    - key: kubernetes.io/hostname
			  	      operator: In
			  	      values:
			  	      - example-node

		Local storage not ideal for database persistence
			tied to a specific node
			won't survive cluster crash		

		hostPath Volume 
			storage on the host node - FOR SINGLE-NODE TESTING ONLY 
				data will only mount/persist for a single node

			apiVersion: v1
			kind: PersistentVolume
			metadata: 
			  name: volume_name
			spec:
			  hostPath: 
			    path: "/mnt/data" #path on node
 			  capacity:
 			    storage: 10Gi
 			  accessModes:
 			  - ReadWriteMany

 		emptyDir Volume
 			shared non-persistent volume for multiple containers in a single pod
 				empty on start and restart
 			defined as volume directly in Pod/Deployment manifest

 			containers:
 			  ...
 			  volumeMounts:
 			  - name: volume_name
 			    mountPath: /path/where/volume/will/be/accessible/in/container
 			volumes:
 			- name: volume_name
 			  emptyDir: {}



	Persistent Volume Claim (PVC)
		also created using YAML file
			kind: PersisitentVolumeClaim
		admin can create multiple generic storage resources, then devs configure claims to access the storage

		Pods use the claim as a volume
		Claim will make use of whatever available resource meets the requirements of the claim
		
		e.g. 
			kind: PersistentVolumeClaim
			apiVersion: v1
			metadata:
			  name: pvc_name
			spec:
			  storageClassName: manual
			  volumeMode: Filesystem
			  accessModes:
			    - ReadWriteOnce
			  resources:
			    requests:
			      storage: 10Gi

		must also use claim in Pod's config volumes: section and volumeMount in containers: section
		
		e.g.
			apiVersion: v1
			kind: Pod
			metadata:
			  name: pod_name
			spec:
			  containers:
			    - name: container_name
			      image: image_name
			      volumeMounts:
			      - mountPath: "/path/to/mount/volume/at"
			        name: volume_name
  			  volumes:
			    - name: volume_name
			      persistentVolumeClaim:
			        claimName: pvc_name

		Volume mounted into Pod and its container(s)
		PVC must be in same namespace as Pod using it
		K8s Admin provisions PV storage resource
		K8s User creates PVC claim to PV resource
			easier for devs (users)

	Storage Class
		abstracts underlying storage provider
		makes PV creation more efficient
		provisions PVs dynamically when PVC claims it

		e.g. for AWS EBS
			apiVersion: storage.k8s.io/v1
			kind: StorageClass
			metadata:
			  name: storage_class_name
			provisioner: kubernetes.io/aws-ebs
			parameters:
			  type: io1
			  opsPerGB: "10"
			  fsType: ext4

		claimed by PVC in YAML under spec:
			storageClassName: storage_class_name

	Special Local Volumes
		for mounting configurations (config files) or secrets (certificates/keys) in pods
		ConfigMap - typically for more extensive and complex information (e.g. database configuration)
		Secrets - typically for small and simple information (e.g. credentials)
		updating content of Secrets/configMaps, does not automatically update content in Pods
			need to restart Pod
			use `kubectl rollout restart deployment/<deployment_name>` for multi-pod deployments

		1) Create ConfigMap and/or Secret component(s)

			ConfigMap
				apiVersion: v1
				kind: ConfigMap
				metadata: 
				  name: configMap_name
				data:
				  key1: value1
				  config_file1: |
				    line1 of configuration
				    line2 of configuration
				    line3 of configuration
				  config_file2: |
				    line1 of configuration
				    line2 of configuration
				    line3 of configuration

			Secret
				apiVersion: v1
				kind: Secret
				metadata:
				  name: secret_name
				type: Opaque
				data:
				  key1: base64_encoded_value1
				  key2: base64_encoded_value2

		2a) Include them in environment variables in Pod/Deployment manifests
			spec: 
			  containers:
			    env:
			    - name: ENV_NAME
			      valueFrom:
			        configMapKeyRef:
			          name: confgMap_name
			          key: key1

			spec:
			  containers:
			    env:
			    - name: ENV_NAME
			      valueFrom:
			        secretKeyRef:
			          name: secret_name
			          key: key1
		AND/OR
		2b) Mount them as volumes in Pod/Deployment manifests
			spec:
			  volumes:
			    - name: config-dir
			      configMap:
			      	name: configMap_name
			
			spec:
			  volumes:
			    - name: secret-dir
			      secret:
			      	secretName: secret_name

			will also need to add these volumes to volumeMounts: for the appropriate container(s)
				should make the mounts `ReadOnly: true`



StatefulSet
	Stateful replica pods are not identical
		can't be created/deleted at same time
		can't be randomly addressed
	StatefulSet maintains "sticky" identity for each pod
		maintained across any re-scheduling
		<statefulset_name>-<ordinal>
			eg. mysql-0, mysql-1, mysql-2
			first is Master
			can't skip numbers in creation/recreation
			must delete in reverse order
	Defines a Master Pod and Worker Pods
		eg. Master can r/w to db, Worker can ro
		don't use the same physical storage
			each have own replica of the storage
			must continuously sync data
				only Master can update data
				Workers must know about each change
			new Worker pods initially clone from other Workers
		if all Pods die, then data would still be lost
			so, use persistent storage
	Configure PV for the StatefulSet
		each Pod has data and Pod state (Master/Worker, etc)
		makes sure PV gets attached to new pod if old one dies
	Each Pod gets its own DNS endpoint as does loadbalancer service
		Service -> svc1
		Pods> mysql-0.svc1, mysql-1.svc1, mysql-2.svc1
		When Pod restarts - IP changes, NOT name & endpoint
	K8s Admin needs to
		configure cloning and data sync
		making remote storage available
		managing and backing up storage
	Stateful apps not ideal for containerised environments


Services
	K8s creates an Endpoint object for each Service
		same name as the Service
		keeps track of the Pods that are its members/endpoints
	Service can have multiple ports open for different connections to its member Pods
		ports must have name attribute in YAML if 2+ ports

	ClusterIP
		most common
		default type
		internal service representing an IP address
		Service's selector attribute(s) and Pods' label attribute(s) define which Pods to fwd requests to
		Service's targetPort attribute defines which Pod ports to fwd to
		randomly selects replicas to loadbalance traffic

	Headless
		allows communication with a specific Pod among replicas
		allows communication between replica Pods
		useful for Stateful apps
			eg. db Master Pod write
			eg. db Worker node sync and clone
		client needs to figure out IPs of each Pod
			DNS lookup usually returns ClusterIP address
			need DNS lookup to return Pod IPs instead
				in Service spec:, set clusterIP: None

	NodePort
		creates static port on Worker Node
		external requests come direct instead of Ingress
		port defined in Service's nodePort attribute
			must be between 30000 and 32767
		auto-creates internal ClusterIP Service with open node defined in NodePort Service's port: attribute
			internal Service present across all replica Worker Nodes
		NOT Secure! - do not use for external connections!

	LoadBalancer
		internal ClusterIP service is made externally accessible via cloud provider's LoadBalancer
		
		each CSP has own LoadBalancer Service

		auto-creates own NodePort and ClusterIP Services
			Node port only communicates with LoadBalancer
			defined in Service's spec: ports:
				port: is ClusterIP port
				nodePort: is Node's port

K8s custom components - plugin interfaces for stuff not default to K8s

	CRI - Container Runtime Interface - allows replacement of Docker with other runtimes

	CNI - Container Network Interface - allow diff forms of networking

	CSI - Container Storage Interface - allows diff storage backends for stateful workloads


Kubernetes Ecosystem
	Prometheus
		monitoring tool - very popular
		provides service IP and pod IPs (typically runs on port 80)

	Jaeger
		tracing/debugging app

	Fluentd
		logging framework
		can read and write logs

	K8s Secrets Management
		Kubernetes Secrets objects (kubectl get secrets)
		Hashicorp Vault 
		CyberArk Conjur
		Square Keywhiz
		Pinterest Knox

	K8s Development Tools
		Visual Studio Code K8s plugin - maintained by MS
			makes use of local kubeconfig file to get access to target clusters
			doesn't require anything to be installed on the target clusters
		Octant from VMWare
			doesn't require anything to be installed on the target clusters




Basic App/Pod Troubleshooting
	Pod running?
		kubectl get pod <pod_name>
	Pod registered with Service and Service forwarding the request?
		kubectl get ep
		kubectl describe <service_name>
	Service accessible?
		nc <service_IP> <service_port>		#from another pod in the cluster
		ping <service_name>					#from another pod in the cluster
	Check app logs
		kubectl logs <pod_name>
	Pod status and recent events
		kubectl describe pod <pod_name>


Troubleshooting with Temporary Pods
	kubectl run debug-pod --image=busybox -it
	
	add command: and args: sections to container spec in manifest for debug-pod
		command: ["<debugging_command_required>" [, "<optionally_args_for_command>"]]
		args: ["<arguments_for_command>"]

		e.g. 
			command: ["printenv"]
			args: ["HOSTNAME", "KUBERNETES_PORT"]

		ENTRYPOINT good for executable that should always run
			defines the process that starts in the container
		CMD good for defaults of an executing container
			provides the arguments for the ENTRYPOINT

	pass command directly to pod and get output to node (not in pod terminal)
		kubectl exec -it debug-pod -- sh -c "command_to_execute"


Formatting Kubectl Output
	-o jsonpath
		allows filtering for specific elements in JSON output
		e.g. get names of all pods
			kubectl get pod -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}'
		e.g. get names and IPs of all pods
			kubectl get pod -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.podIP}{"\n"}{end}'
		e.g. get names of all pods and their start times
			kubectl get pod -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.startTime}{"\n"}{end}'

	-o custom-columns
		custom column titles
		e.g. get names and IPs of all pods in neat, labelled columns
			kubectl get pod -o custom-columns=POD_NAME:.metadata.name,POD_IP:.status.podIP

Troubleshooting Kubelet
	1) ssh into affected node (e.g. status NotReady)
	2) check service status
		service kubelet status
	3) check context logs
		journalctl -u kubelet
	4) restart service
		systemctl daemon-reload
		systemctl restart kubelet

Troubleshooting Kubectl
	1) Check kubeconfig file
		a) check decoded cluster certificate data against /etc/kubernetes/pki/ca.crt
		b) check server endpoint against IP of control-plane node












