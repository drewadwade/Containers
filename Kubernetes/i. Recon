Nmap
	Watch out for unauthenticated services 
		esp MongoDB
		sim internal network review

	API remote port can be any (configurable on launch)
	443 - API remote secure port - common

	2379 - etcd client communication
	2380 - etcd inter-cluster communication

	6443 - API remote default secure port 
	8080 - API remote default insecure port
	8443 - API remote secure port - common

	10249 - sometimes kube-proxy (see 10256)
	10250 - kubelet
			read-write
			kubelet API endpoints - https://github.com/cyberark/kubeletctl/blob/master/API_TABLE.md
	10251 - Controller Manager 
			metrics and health info (unauth disclosure finding)
	10255 - kubelet (older) 
			read-only
			kubelet API endpoints - https://github.com/cyberark/kubeletctl/blob/master/API_TABLE.md
	10256 - kube-proxy 
			handles mapping services to pods
			forwards traffic to containers
			health and metrics info

	30000-32767 - node port services, including Tiller

DNS
	coredns on default kubeadm clusters listens on 10.96.0.10

	List all service DNS records with their corresponding svc IP
		dig +short <server_name> any.any.svc.cluster.local

		doesn't give service names, but does give list of IPs for targetted portscans


Indicators you're in a pod
	/etc/hosts
		# Kubernetes-managed hosts file._127.0.0.1_localhost_::1_localhost ip6-localhost ip6-loopback_fe00::0_ip6-localnet_fe00::0_ip6-


Kubernetes Servers on the Internet
	a lot of managed Kubernetes vendors make the API server Internet-accessible by default
		should be disabled if not needed!

	How do we know we’ve found a Kubernetes server?
		predictable and consistent TLS certificate SANs

	What version of Kubernetes is running?
		many clusters will make the /version endpoint available without authentication
			should be disabled if not needed!
			Microsoft blocks by default

	What vendor is running this cluster?
		many of the clusters have customized version strings (see /version)
			‘gke’ for GKE
			? for EKS
			? for IKS
			hex string at end for OpenShift (e.g. v1.9.1+a0ce1bc657)


Query Secure Port
	kubectl -s http://<node_IP>:8443 <command>
	OR
	curl -k https://10.129.173.36:8443/api/v1/namespaces   

Query Insecure Port
	kubectl -s http://<node_IP>:8080 <command>

Query unauthenticated rw kubelet API
	curl https://172.18.0.4:10250/pods -k | jq
	
Query unauthenticated ro kubelet API
	curl http://172.18.0.4:10255/pods -k | jq
		run and exec endpoints will return 405

Query unauthenticated etcd
	export ETCDCTL_API=3

	etcdctl --insecure-skip-tls-verify --insecure-transport=false --endpoints=172.18.0.4:2379 get / --prefix --keys-only

Check for Tiller
	helm --host <target_IP>:<tiller_port> version

	from a pod in the cluster:
		dig tiller-deploy.kube-system.svc.cluster.local
			if we get a response, then Tiller is installed cluster-wide in kube-system namespace


List API resources
	kubectl api-resources | grep -v "NAME" | cut -d " " -f1

List namespaces (defaults = default, kube-public, kube-system)
	kubectl get namespaces | grep -v "NAME" | cut -d " " -f1

For each accessible namespace gather info
	kubectl get po,svc,roles,rolebindings,clusterroles,clusterrolebindings,networkpolicies,psp,no,ns,pv,pvc,rc,crds,ds,deploy,rs,sts,ing --all-namespaces -o wide

For each namespace gather permissions
	kubectl -n <namepsace> auth can-i --list

Find and read any available secrets
	kubectl get secrets
	kubectl get secret <SECRET_NAME> -o yaml 

	Get service account token "default-token-..."
		/var/run/secrets/kubernetes.io/serviceaccount/token		
		created by default in all containers
		in all older versions and some newer versions, this is a cluster-admin token

	look for mounted secrets in tmpfs file systems on container
		cat /proc/mounts | grep tmpfs
			check files within any mounts that look like they may be for secrets 

	with access to etcd database backup
		strings db | grep -B 4 Opaque
		strings db | grep -B 1 -A 1 "service-account-token"

	in environment variables
		env 


List capabilities for a container
	Run amicontained in the container (also gets AppArmor Profile, seccomp, namespaces)
		curl -LO k8s.work/amicontained
		chmod +x amicontained
		./amicontained


	Find container process (e.g. bash)
		ps 	
	Get capabilities the the process
		getpcaps <PID>




Check Service Account Permissions
	often the default service account is given too many rights
		new pods will get default service account by default

	kubectl auth can-i --as <subject> <kubectl_command>
		if running as cluster-admin, will return yes regardless (rights is *)
			`can-i get donuts` will return error but true


Identify authenticated API with Basic Auth enabled
	curl and pass any creds to API server as above
		403 - not enabled
		401 - enabled, user/pass incorrect
	if no account lockout, can bruteforce


Inside a pod
	Gather k8s environment variables
		env | grep -i kube

		created in every container
			KUBERNETES_PORT
			KUBERNETES_PORT_443_TCP_ADDR
			KUBERNETES_SERVICE_HOST

	If service account token available
		Get API-Server IP
			Get pod IP - API will likely be at base of that IP range
		Get JWT access token 
			kubectl get secret default-token-...
			OR 
			/var/run/secrets/kubernetes.io/serviceaccount/token		
		Create TOKEN variable for ease of use
			$TOKEN="<decoded JWT>"
		Test access to API server
			curl -X GET https://<API_SERVER_IP>/api --header "Authorization: Bearer $TOKEN" --insecure



Check for Basic Auth
	curl -sk --user user1:mypass https://<IP>:<port>/api/...

	identify servers with Basic Auth enabled
			curl and pass creds to API server as above
				403 - not enabled
				401 - enabled, user/pass incorrect
			no account lockout - can bruteforce if enabled





Testing creds usually come as .kube/config file
	look for these in dev systems

Check repos for CA key files
	provides access to whole cluster for life of key (10y by default)

Check RBAC rights applied to clusters
	esp. 3rd-party intalls - sim to dangers of curl-bashing
	install may do things like bind default service account to cluster-admin

Manual Permission Auditing
	Get members of clusterroles (sort of)
		kubectl get clusterrole cluster-admin -o yaml
		kubectl get clusterrolebinding cluster-admin -o yaml
		still won't show mention of users

	Check permissions for current user
		kubectl auth can-i <kubectl_command>
			e.g. kubectl auth can-i get secrets
	Check permissions for other users/subjects
		kubectl auth can-i --as <subject> <kubectl_command>
		if running as cluster-admin, will return yes regardless (rights is *)
			`can-i get donuts` will return error but true

	List all permissions for current user
		kubectl auth can-i --list
	List all permissions for other users/subjects
		kubectl auth can-i --as <subject> --list

Automated Permission Auditing
	rakkess (github.com/corneliusweig/rakkess)	
		matrix of resources
		show all resources and permissions
			rakkess 			
		show resource access for specific service account
			rakkess	--sa <service_acount_name>
	kubectl-who-can (github.com/aquasecurity/kubectl-who-can)
		which users can do something
			kubectl-who-can <kubectl_command>
				e.g. kubectl-who-can get secrets
				e.g. kubectl-who-can create pods
		look for nonstandard bindings and investigate those
		useful for privesc
			can spot accounts with more access
				e.g. system:controller:clusterrole-aggregation-controller
					service account with admin level rights 
						used to be cluster-admin
						now can escalate to cluster-admin
			can see who has get secrets permission
				accesss to the service token of above would effectively allow cluster-admin rights

	rback (github.com/mhausenblas/rback)
		visualisation of RBAC rules
			rback | dot -Tpng > /tmp/rback.png
		may be broken for recent K8s

Custom Resources
	list resources
		kubectl api-resources


Finding services running (to check for unauth access, etc)
	kubectl svc --all-namespaces

If you have a multi-tenant cluster, with tenants divided by namespace, they will still be able to brute-force discover one another's pods (especially if the namespace is something simple like the tenant company's name). They won't necessarily be able to access one another's pods, but there is some information leakage in the form of DNS entries. 


K8s CIS Benchmarks
	targets a kubeadm cluster as a baseline
		take care applying to other forms of K8s
	vendor-specific benchmarks coming out - EKS, GKE
	can audit:
		locally on cluster nodes - preferable
		remotely via cluster API - not all functionality available

	Kube-bench (github.com/aquasecurity/kube-bench)
		sim Docker-Bench
		audits locally - requires shell access to Master Node (therefore can't use with managed K8s)
			customer could run it themselves and provide us the output
		typically up to date with a relatively recent CIS benchmark version

		on cluster master node run:
			docker run --rm -v `pwd`:/host aquasec/kube-bench:latest install
			docker cp kube-bench <node_name>:/
			docker cp cfg/ <node_name>:/
			docker exec -it <node_name> bash
		in node shell run:
			./kube-bench master

		will pick up Low/Info findings like:
			CIS 1.1.1  --anonymous-auth not set to false
				should be disabled, but may break monitoring tools

		"Warn" means it couldn't check this automatically


K8s Config Testing Tools
	github.com/raesene/TestingScipts/offline-cluster-analyzer.rb
		run included kubectl commands to gathers information on most resources in JSON for offline analysis
			no sensitive information (no secrets)
			client could run kubectl command and provide output
				can be very useful for scoping!
		parse JSON with offline-cluster-analyzer.rb


	Kind (github.com/kubernetes-sigs/kind)
		K8s in Docker, sim DIND
		creates K8s nodes inside Docker containers 
		makes use of privileged containers for nested docker
		not suitable for production use!
			good for demos - easy to set up

	KubiScan (github.com/cyberark/KubiScan)
		looks for dangerous RBAC rights

	kubeletctl (github.com/cyberark/kubeletctl)
		automates curl testing against kubelet API
			esp useful on unauthenticated kubelet API		 


K8s PT Tools
	kube-hunter (github.com/aquasecurity/kube-hunter)
		git clone https://github.com/aquasecurity/kube-hunter.git
		cd kube-hunter
		sudo apt install python3-pip
		pip3 install -r requirements.txt
		./kube-hunter.py

		will make calls out to Azure in process of testing


AWS K8s
	Initiate Connection
		aws configure
			provide Access Key ID, Secret Access Key, Region 
		aws eks update-kubeconfig --name <EKS_resource_name> --profile <your_profile_name>

	Collect Information
		kubectl get po,svc,roles,rolebindings,clusterroles,clusterrolebindings,networkpolicies,psp,no,ns,pv,pvc,rc,crds,ds,deploy,rs,sts,ing --all-namespaces -o json > clusterconfig.json

		./offline-cluster-analyzer.rb -i ./clusterconfig.json

	List Images 
		kubectl get pods --all-namespaces -o jsonpath="{..image}" | tr -s '[[:space:]]' '\n' | sort -u > pods

	Check Images for CVEs
		#! /bin/bash
		cat ./pods | while read LINE; do
		    trivy image $LINE >> images
		done

	Get helm/tiller version (from in a pod on the system)
		helm --host tiller-deploy.kub-system.svc.cluster.local:44134 version





